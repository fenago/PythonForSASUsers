{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 09 -- Panda Time Series and Date Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics Covered:\n",
    "\n",
    "<a href=\"http://nbviewer.jupyter.org/github/RandyBetancourt/PythonForSASUsers/blob/master/Chapter%2009%20--%20Panda%20Time%20Series%20and%20Date%20Handling.ipynb?flush_cache=true/#Creating-and-manipulating-a-fixed-frequency-of-dates-and-time-spans\"> Creating and manipulating a fixed-frequency of dates and time spans </a>\n",
    "\n",
    "<a href=\"http://nbviewer.jupyter.org/github/RandyBetancourt/PythonForSASUsers/blob/master/Chapter%2009%20--%20Panda%20Time%20Series%20and%20Date%20Handling.ipynb?flush_cache=true/#Time-Series-Walk-Through\">Time Series Walk-Through</a>\n",
    "\n",
    "<a href=\"http://nbviewer.jupyter.org/github/RandyBetancourt/PythonForSASUsers/blob/master/Chapter%2009%20--%20Panda%20Time%20Series%20and%20Date%20Handling.ipynb#Returning-Unique-Levels-of-Categories\">Returning Unique Levels of Categories</a>\n",
    "\n",
    "<a href=\"http://nbviewer.jupyter.org/github/RandyBetancourt/PythonForSASUsers/blob/master/Chapter%2009%20--%20Panda%20Time%20Series%20and%20Date%20Handling.ipynb#Return-a-Row-using-a-Minimum-Value\">Return a Row using a Minimum Value</a>\n",
    "\n",
    "<a href=\"http://nbviewer.jupyter.org/github/RandyBetancourt/PythonForSASUsers/blob/master/Chapter%2009%20--%20Panda%20Time%20Series%20and%20Date%20Handling.ipynb#Return-a-Row-using-a-Maximum-Value\">\n",
    "Return a Row using a Maximum Value</a>\n",
    "\n",
    "<a href=\"http://nbviewer.jupyter.org/github/RandyBetancourt/PythonForSASUsers/blob/master/Chapter%2009%20--%20Panda%20Time%20Series%20and%20Date%20Handling.ipynb?flush_cache=true/#Convert-Time-Series-from-one-Frequency-to-Another\">Convert time series from one frequency to another</a>\n",
    "\n",
    "<a href=\"http://nbviewer.jupyter.org/github/RandyBetancourt/PythonForSASUsers/blob/master/Chapter%2009%20--%20Panda%20Time%20Series%20and%20Date%20Handling.ipynb#Resources\">Resources</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 8, <a href=\"http://nbviewer.jupyter.org/github/RandyBetancourt/PythonForSASUsers/blob/master/Chapter%2008%20--%20Python%20Date%2C%20Time%2C%20and%20%20Timedelta%20Objects.ipynb\"> Understanding Date Time and TimeDelta objects </a> provided a short introduction to Python's built-in datetime capabilities.  In this chapter we illustrate pandas time series and date handling.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, time, datetime, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame, Index\n",
    "from numpy.random import randn as rnd\n",
    "from IPython.display import Image\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and manipulating a fixed-frequency of dates and time spans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pd.date_range() method generates a DateTime Index which is applied to a panda Series or DataFrame to provide datetime interval indexing.  We will see examples of its construction methods.  And later we will utilize indexers taking advange of the Date TimeIndex.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = pd.date_range('1/1/2016', periods=30, freq='D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first 10 dates in the DateTimeIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Assemble a Series by using strings and integers for columns.  Map the year, month, and day value into a date timestamp using the pd.to_datetime() method.  Details for the pd.datetime() method are found <a href=\"http://pandas.pydata.org/pandas-docs/version/0.18.1/generated/pandas.to_datetime.html\"> here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'year': ['2014', '2015', '2016'],\n",
    "                   'month': [1, 2, 3],\n",
    "                   'day': [1,2,3,]})\n",
    "df1 = pd.to_datetime(df)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the Series 'b_rng' containing only business days using the pd.bdate() method and supplying start and end date.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(2016, 1, 1)\n",
    "end = datetime(2016, 12, 31)\n",
    "b_rng = pd.bdate_range(start,end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Series implicitly creates a DatetimeIndex object.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(b_rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_rng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Series containing the last business day of the month for 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = pd.date_range(start, end, freq='BM')\n",
    "ts = pd.Series(np.random.randn(len(rng)), index=rng)\n",
    "ts.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the first 5 last business day of the month for 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts[:5].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Returns the last business day of every other month in 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts[::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Walk-Through\n",
    "\n",
    "We can begin combining features covered in previous chapters to conduct a walk-through of a simple   time series analysis.  \n",
    "\n",
    "#### The Data\n",
    "\n",
    "The data is the FHFA House Price Index (HPI) which is a broad measure of the movement of single-family house prices. It is a weighted, repeat-sales index, meaning that it measures average price changes in repeat sales or refinancing on the same properties. This information is obtained by reviewing repeat mortgage transactions on single-family properties whose mortgages have been purchased or securitized by Fannie Mae or Freddie Mac.  \n",
    "\n",
    "Details about the data and how it is organized can be found <a href=\"https://catalog.data.gov/dataset/fhfa-house-price-indexes-hpis\"> here </a>. This time series begins January 1991 and end August 2016.  Both the seasonally adjusted index 'index_sa' and the non-seaonally adjusted index 'index_nsa' set the index value at 100 for January 1991.  \n",
    "\n",
    "#### The Inquiry\n",
    "\n",
    "The three salient questions to answer are:\n",
    "\n",
    "    1. Have U.S. aggregate home prices recovered their value since the Great Recession of 2008-2010? \n",
    "    2. Where are the highest and lowest values for 'traditional', 'purchase-only' homes in the U.S. in 2016? \n",
    "    3. How do the highest and lowest home values market segments compare to the aggregate U.S. home prices?\n",
    "    \n",
    "#### The Approach\n",
    "\n",
    "The input .csv file located <a href=\"some_URL\"> here </a>.  \n",
    "\n",
    "The file is composed of two parts.  Part 1, rows 2 to 3081 are records for the aggregate market groups at the Census Division level.  The frequency interval is monthly.  \n",
    "\n",
    "This portion of the file has 10 columns containing values for major market segments and the U.S. aggregate prices.  The price indicies are both seasonally adjusted and non-seasonally adjusted values.  The end-result DataFrame will be called 'df_us'.\n",
    "\n",
    "Part 2, rows 3082 to 96,243 are more granular with a quarterly frequency interval. The major U.S. market segments are broken into smaller geographics areas with just the non-seasonaly adjusted home index value. Accordingly, the 10th column 'index_sa' contains no values at this location to the end of the file. The end-result DataFrame will be called 'df_states'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1 of .csv File to Construct the 'df_us' DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with a test read of the entire .csv file.  The pd.read_csv method has the one required arguement, the input file name to create the DataFrame 'df_all'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(\"data/HPI_master.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the first 5 rows to determine if the read_csv() method is giving the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to combine the year and period fields into a <a href=\"http://nbviewer.jupyter.org/github/RandyBetancourt/PythonForSASUsers/blob/master/Chapter%2008%20--%20Date%2C%20Time%2C%20and%20%20Timedelta%20Objects.ipynb#String-Literal-Mapped-to-datetime-timestamp\"> datetime timetamp </a>.  The .csv file in the cell above is read without any datetime parsing for the fields, 'yr' and 'period'.  We could post-process these fields to construct the appropriate date timestamp values.  \n",
    "\n",
    "A better approach is below.  The parse_dates= argument allows a <a href=\"http://nbviewer.jupyter.org/github/RandyBetancourt/PythonForSASUsers/blob/master/Chapter%2002%20--%20Data%20Structures.ipynb#dictionary\"> dictionary </a> object with the key being the arbitrary name of the new column created and the key values indicating which fields are to be read in the .csv file.  Recall that Python indexes have a start position of 0.  In the .csv file, these fields are the 7th and 8th column position.\n",
    "\n",
    "Sometimes, you may need to create your own date-parser, analogous to building a user-defined SAS INFORMAT to map field values into a datetime object.  This is particularly true in cases where the date value is stored as component values in multiple fields.   \n",
    "\n",
    "The nrows= argument value is set to 3080.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us = pd.read_csv(\"data/HPI_master.csv\",\n",
    "                 parse_dates={'date_idx': [6,7]},\n",
    "                 nrows=3080)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate column names and their data types.  Confirm the date parser constructed the column 'date_idx' as a datetime object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the 'date_idx' column as the <a href=\"http://nbviewer.jupyter.org/github/RandyBetancourt/PythonForSASUsers/blob/master/Chapter%2005%20--%20Understanding%20Indexes.ipynb#Setting-and-resetting-Indicies\">index</a> on the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us.set_index(\"date_idx\", inplace=True, drop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing on the datetime column 'date' creates a 'datetime-aware' DateTimeIndex.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the earlies and lastest date values in the 'df_us' DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Earliest date is:', df_us.date_idx.min())\n",
    "print('Latest date is:', df_us.date_idx.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from the list of columns returned from the .info() attribute above we have several categorical columns.  We need to understand their levels.  Earlier, we saw the .describe() method used for numerical columns.  In the example below specifying the 'include=' argument returns a description of string columns.  \n",
    "\n",
    "Recall that dtype 'O' (not zero) indicates string values for a Series or DataFrame column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'place_name' column has 10 unique levels or values.  We can examine these values with the .unique() attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us.place_name.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue by setting an <a href=\"http://nbviewer.jupyter.org/github/RandyBetancourt/PythonForSASUsers/blob/master/Chapter%2005%20--%20Understanding%20Indexes.ipynb#Setting-and-resetting-Indicies\"> index </a> on the column 'place_name'.  Construct the 'df_us_plot' DataFrame with rows for 'place_name' equal to 'United States'.  The .loc indexer allows row slicing which is covered in detail <a href=\"http://nbviewer.jupyter.org/github/RandyBetancourt/PythonForSASUsers/blob/master/Chapter%2005%20--%20Understanding%20Indexes.ipynb#.loc-Indexer\">here</a>.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us.set_index('place_name', inplace=True, drop=False)\n",
    "df_us_plot = df_us.loc['United States']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer to question 1\n",
    "    Have U.S. aggregate home prices recovered their value since the Great Recession of 2008-2010?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the Great Recession of 2008-2010, home prices across the U.S. declined dramatically.  The aggregate U.S. home price index has recovered all of the losses since that time and have continued to experience steady growth. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the DataFrame 'df_us_3' to select the rows with the values indicated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_3 = df_us.loc[['West South Central Division', 'United States', 'Pacific Division']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2 of .csv File to Construct the 'df_states' DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use to 'skiprows=' argument to begin reading at row 3082.  We use a tuple to specify the columns labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning with row 3082 the values for the field 'index_sa' are missing.  Begin the read at row 3082 until end of file.  And since the default is to key off column names, supply column labels with a<a href= \"http://nbviewer.jupyter.org/github/RandyBetancourt/PythonForSASUsers/blob/master/Chapter%2002%20--%20Data%20Structures.ipynb#tuple\"> tuple</a> of names.  The usecols= argument uses a tuple of integers to indicate which fields are to be read from the .csv file.  Header=None is to prevent the reader from building column names at row position nrows-1, which in our case contains data values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_states = pd.read_csv(\"data/HPI_master.csv\",                 \n",
    "            skiprows=3082,\n",
    "            usecols=(0, 1, 2, 3, 4, 5, 6, 7, 8),\n",
    "            names=('hpi_type', 'hpi_flavor', 'frequency', 'level', 'place_name', 'place_id', 'yr', 'period', 'index_nsa'),\n",
    "            header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns 'yr' and 'period' are read as string values and need to be converted to datetime values.  The operation below creates the new column 'date_str' by:\n",
    "\n",
    "    1. Concatenating 'yr' with 'Q' with 'period' to form a date string \n",
    "    2. The date string is passed to the pd.to_datetime function creating the dateime column 'date_idx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. String concatenation operation to form YYYYq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_states[\"date_str\"] = df_states['yr'].map(str) + 'Q' + df_states['period'].map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the 'date_str' value using the <a href=\"http://nbviewer.jupyter.org/github/RandyBetancourt/PythonForSASUsers/blob/master/Chapter%2005%20--%20Understanding%20Indexes.ipynb#.iloc-Indexer\"> iloc indexer</a> which returns row and column location by integer positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_states.iloc[0,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. Convert the 'date_str' column into a panda datetime column called 'date_idx'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_states['date_idx'] = pd.to_datetime(df_states['date_str'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the first 5 records in the 'df_states' DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_states.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the number of rows and columns in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_states.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following SAS Data Step reads the same .csv file using FIRSTOBS= to begin reading from the arbitary row position 3082."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````\n",
    "    /********************************/\n",
    "    /* c09_read()_csv_df_states.sas */\n",
    "    /********************************/\n",
    "    data df_states;\n",
    "          infile 'data/HPI_master.csv' delimiter=',' missover dsd firstobs=3082; \n",
    "          informat hpi_type $12.\n",
    "                   hpi_flavor $16.\n",
    "                   frequency $9.\n",
    "                   level $28.\n",
    "                   place_name $33.\n",
    "                   place_id $8.\n",
    "                   yr $5.\n",
    "                   period $6.\n",
    "                   index_nsa 8.;\n",
    "             input hpi_type $\n",
    "                   hpi_flavor $\n",
    "                   frequency $\n",
    "                   level $\n",
    "                   place_name $\n",
    "                   place_id $\n",
    "                   yr $\n",
    "                   period $\n",
    "                   index_nsa ;\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first 5 rows of the SAS data set df_states.  In the SAS code example below the 'yr' and 'period' variables are combined to create the SAS datetime variable 'date_idx'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='output/df_states_output.JPG')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect values for the categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_states.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returning Unique Levels of Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get unique values for column 'level'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_states.level.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROC SQL used to obtain unique values from the variable 'level'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````\n",
    "    /******************************************************/\n",
    "    /* c09_select_unique_level.sas                        */\n",
    "    /******************************************************/\n",
    "    56       proc sql;\n",
    "    57          select unique level\n",
    "    58          from df_states;\n",
    "    59       quit;\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='output/level_unique_values.JPG')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by creating a <a href=\"http://nbviewer.jupyter.org/github/RandyBetancourt/PythonForSASUsers/blob/master/Chapter%2002%20--%20Data%20Structures.ipynb#list\">list</a> called 'lvls' using the .select_dtypes() attribute to include columns with dtype value 'O' selecting columns with string values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvls = list(df_states.select_dtypes(include=['O']).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the 'lvls' list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had a long list of columns needing to determine unique values the approach above becomes tedious; an iterative approach is called for using the for statement shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in lvls[:4]:\n",
    "   print(item, 'levels are:', df_states[item].unique())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the levels for the categorial columns displayed above, filter the 'df_state' DataFrame to match the 'df_us' Dataframe.  For this row slicing operation, create a boolean mask and combine it with the .loc indexer described <a href=\"http://nbviewer.jupyter.org/github/RandyBetancourt/PythonForSASUsers/blob/master/Chapter%2005%20--%20Understanding%20Indexes.ipynb#Mixing-.loc-Indexer-with-Boolean-Operators\">here</a>.  This is a common pattern for filtering rows or column values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a boolean mask for the filtering criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_states['hpi_type'] == 'traditional') & (df_states['hpi_flavor'] == 'purchase-only') & \\\n",
    "       (df_states['level'] == 'State')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the mask.  Notice the 'df_states' DataFrame is updated in place with this assignment.  The original 'df_states' DataFrame had 96244 rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_states = df_states.loc[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .shape attribute returns the new row and column count for the 'df_states' DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_states.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimums and Maximums for Part 2, 'df_states' DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index the column 'index_nsa' in order to find the maximum and minimum for the 'df_states' DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_states.set_index('index_nsa', inplace=True, drop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Max value for index_nsa:', df_states['index_nsa'].max())\n",
    "print('Min value for index_nsa:', df_states['index_nsa'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROC SQL for finding min and max for the variable 'index_nsa'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````\n",
    "    /******************************************************/\n",
    "    /* c09_min_max_index_nsa.sas                          */\n",
    "    /******************************************************/\n",
    "37      proc sql;\n",
    "38         select max (index_nsa) as max_index_nsa,\n",
    "39                min (index_nsa) as min_index_nsa\n",
    "40         from df_states\n",
    "41         where hpi_type ='traditional' and  hpi_flavor = 'purchase-only' and\n",
    "42          level = 'State';\n",
    "43      quit;\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='output/max_min_index_nsa.JPG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter for  2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 2016, return the rows with the largest and smallest value for 'index_nsa'.  \n",
    "\n",
    "In order to find the row with the lowest 'index_nsa' value, the example below uses a multi-step process:\n",
    "    1. Create a boolean mask filtering the rows with 'date_idx' between January 1, 2016 and December 31, 2016\n",
    "    2. Use the .loc indexer applying the mask to create the 'df_2016' DataFrame\n",
    "    3. Use the .idxmin() attribute to return the row having the minimum 'index_nsa' value\n",
    "    4. Use the .idxmax() attribute to return the row having the maximum 'index_nsa' value\n",
    "    \n",
    "Keep in mind that the 'df_states' DataFrame has already been filtered previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the boolean mask for the filtering criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = (df_states['date_idx'] >= '2016-01-01') & (df_states['date_idx'] <= '2016-12-31')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the mask using the <a href=\"http://nbviewer.jupyter.org/github/RandyBetancourt/PythonForSASUsers/blob/master/Chapter%2005%20--%20Understanding%20Indexes.ipynb#.loc-Indexer\">.loc indexer</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2016 = df_states.loc[mask1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return a Row using a Minimum Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the .idxmin() method to return the minimum 'index_nsa' value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2016.ix[df_2016['index_nsa'].idxmin()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return a Row using a Maximum Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the .idxmax() attribute to return the row with the maximum 'index_nsa' value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2016.ix[df_2016['index_nsa'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .min and .max attribute return minimum and maximum respectively.  The .idxmin() attribute returns the entire row as a Series.  This enables the ability to use other column values, like 'place_name' or 'place_id' as further filtering and selection criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .min() and .max attributes return a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2016 minimum value for 'index_nsa':\", df_2016.index_nsa.min())\n",
    "print(\"2016 maximum value for 'index_nsa':\", df_2016.index_nsa.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SAS Data Step above used to the read .csv file does not create a SAS datetime variable.  This is illustrated below. \n",
    "\n",
    "The SAS example below uses a pair of PUT functions nested inside the YYQ function to create the SAS date variable 'date_idx'.  The PUT functions map the variables 'yr' and 'period' from character to numeric.  The YYQ function described <a href=\"http://support.sas.com/documentation/cdl/en/lrdict/64316/HTML/default/viewer.htm#a000199346.htm\">here</a> returns a SAS datetime values from year and quarter values.\n",
    "\n",
    "The Data Step below is continued from the SAS Data Step example above used to read the .csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````\n",
    "    /******************************************************/\n",
    "    /* c09_min_max_index_nsa_for2016.sas                  */\n",
    "    /******************************************************/\n",
    "    12      date_idx=yyq(put(yr,8.),put(period,8.));\n",
    "    13      format date_idx yyq10.;\n",
    "    14      \n",
    "    15      proc sql;\n",
    "    16         select max (index_nsa) as max_index_nsa,\n",
    "    17                min (index_nsa) as min_index_nsa\n",
    "    18         from df_states\n",
    "    19         where hpi_type ='traditional' and\n",
    "    20               hpi_flavor = 'purchase-only' and\n",
    "    21               level = 'State' and\n",
    "    22               date_idx between '01Jan2016'd and '31Dec2016'd;\n",
    "    23      quit;\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='output/2016_min_max.JPG')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer to question #2\n",
    "\n",
    "    2. Where are the highest and lowest values for 'traditional', 'purchase-only' homes in the U.S. in 2016? \n",
    "    \n",
    "**Lowest home prices are in the state of Connecticut** and the **highest are in Washington, D.C.**  \n",
    "\n",
    "Recall the 'df_states' DataFrame was filtered for column 'hpi_type' = 'traditional' **and** column 'hpi_flavor' = 'purchase-only' **and** level='state'.  This DataFrame was subsequently filtered to create the DataFrame 'df_2016' in which we searched for the lowest and highest home index values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Time Series from one Frequency to Another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to answer the last question:\n",
    "    How do the highest and lowest home values market segments compare to the aggregate U.S. home prices?\n",
    "    \n",
    "Information from the 'df_states' and 'df_us' DataFrame need to be combined.  The 'df_us' monthly values need to be downsampled to quarterly.  In other words, aggregated to a lower frequency.  In doing so, the frequency in both DataFrames become quarterly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to answer the 3rd question above, we need to conduct the following:\n",
    "    1. Set the index for 'df_us' DataFrame to the column 'date_idx'\n",
    "    2. Resample the 'df_us' DataFrame aggregating from monthly to quarterly using mean values\n",
    "    3. Create a column in the 'df_us' labeled 'place_name' with the value \"U.S. Aggregate\".  \n",
    "    4. Extract rows from 'df_states' DataFrame for lowest and highest home values using the column 'place_name' \n",
    "    5. Merge (concatenate) the resampled 'df_us' DataFrame with the high & low value rows from the extract operation\n",
    "    6. Plot the resulting 'DataFrame'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by examining the quarterly date values from the 'df_states' DataFrame... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df_states.iloc[0:4, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and compare with the monthly date values from the 'df_us' DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us.iloc[0:4, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Set the index for 'df_us' DataFrame to the column 'date_idx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us.set_index('date_idx', inplace=True, drop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    2. Resample the 'df_us' DataFrame aggregating from monthly to quarterly using mean values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas can perform resampling operations during frequency conversion (in this case, an aggregation of monthly values into quarterly values for the 'df_us' DataFrame). This is a common pattern in time series analysis.\n",
    "\n",
    "The .resample() method is a time-based <a href=\" \">needs a link to groupby</a> operation, followed by a reduction method on each of its groups.  Frequency conversion doc is found <a href=\"http://pandas.pydata.org/pandas-docs/stable/timeseries.html#frequency-conversion\"> here</a>.  The .resample() method accepts frequency offset suffixes that are listed <a href=\"http://pandas.pydata.org/pandas-docs/stable/timeseries.html#anchored-offsets\"> here</a>. \n",
    " \n",
    "In order to align the monthly date values found in the 'df_us' DataFrame, with the quarterly date values in the 'df_states' DataFrame, use the 'QS' date offset.  'QS' sets the date value frequency to quarterly with year ending in December.  The resulting DataFrame is called 'df_us_qtr'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_qtr = df_us.resample('QS').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create a column in the 'df_us' labeled 'place_name' with the value \"U.S. Aggregate\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_qtr['place_name'] = 'U.S. Aggregate'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Extract rows for the lowest and highest home values using the df_us_qtr['place_name'] column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Set the <a href=\"http://nbviewer.jupyter.org/github/RandyBetancourt/PythonForSASUsers/blob/master/Chapter%2005%20--%20Understanding%20Indexes.ipynb#Setting-and-resetting-Indicies\">index</a> to the column 'place_name' and extract the rows using the <a href=\"http://nbviewer.jupyter.org/github/RandyBetancourt/PythonForSASUsers/blob/master/Chapter%2005%20--%20Understanding%20Indexes.ipynb#.loc-Indexer\">.loc indexer</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_states.set_index('place_name', inplace=True, drop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a boolean mask using the logical OR operator ( | )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df_states['place_name'] == 'Connecticut') | (df_states['place_name'] == 'District of Columbia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the mask using the <a href=\"http://nbviewer.jupyter.org/github/RandyBetancourt/PythonForSASUsers/blob/master/Chapter%2005%20--%20Understanding%20Indexes.ipynb#.loc-Indexer\">.loc indexer</a> to create the 'hi_lo' DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_lo = df_states.loc[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://nbviewer.jupyter.org/github/RandyBetancourt/PythonForSASUsers/blob/master/Chapter%2005%20--%20Understanding%20Indexes.ipynb#Setting-and-resetting-Indicies\">Reset the index</a> since the 'hi_lo' DataFrame will be concatenated (merged) with the resampled 'us_df' DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_lo.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Merge (concatenate) the resampled 'df_us' DataFrame with the high & low value rows from the extract operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the index, since the 'df_us_qtr' DataFrame will be merged with the 'hi_lo' DataFrame created above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_qtr.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a <a href=\"http://nbviewer.jupyter.org/github/RandyBetancourt/PythonForSASUsers/blob/master/Chapter%2002%20--%20Data%20Structures.ipynb#list\">list</a> of DataFrames to be included in the concatenation operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_us_qtr, hi_lo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer to Question 3.\n",
    "    3. How do the highest and lowest home values market segments compare to the aggregate U.S. home prices?\n",
    "\n",
    "In the plot above, we see that the aggregate U.S. home price index regained its losses from the Great Recession (2008-2010) beginning around 2015.  Since then it has seen steady growth and has exceeded the peak from the pre-recession values.\n",
    "\n",
    "In contrast, the home value index for Connecticut is just now recovering its value lost during the Great Recession.  However, it has not recovered its pre-recession value.\n",
    "\n",
    "Since 2011, the aggregate U.S. home price index has been growing faster than that of Connecticut, but not even close to the growth rate for Washington, D.C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources \n",
    "\n",
    "pandas Time Series and Date functionality doc located <a href=\"http://pandas.pydata.org/pandas-docs/stable/timeseries.html\">here</a>.\n",
    "\n",
    "pandas datetime Indexing doc located <a href=\"http://pandas.pydata.org/pandas-docs/stable/timeseries.html#datetime-indexing\">here</a>.\n",
    "\n",
    "pandas cookbook for <a href=\"http://pandas.pydata.org/pandas-docs/stable/cookbook.html#timeseries\">timeseries</a>.\n",
    "\n",
    "Chapter 10, Time Series, \"Python for Data Analysis, by Wes McKinney, located <a href=\"http://shop.oreilly.com/product/0636920023784.do\">here</a>.\n",
    "\n",
    "SAS 9.4 Language Reference: Concepts, 5th ed., <a href=\"http://support.sas.com/documentation/cdl/en/lrcon/68089/HTML/default/viewer.htm#n0q9ylcaccjgjrn19hvqnd9cte8p.htm\"> Dates, Times, and Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Navigation\n",
    "\n",
    "<a href=\"http://nbviewer.jupyter.org/github/RandyBetancourt/PythonForSASUsers/tree/master/\"> Return to Chapter List </a>    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
